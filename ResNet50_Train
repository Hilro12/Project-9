import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import matplotlib.pyplot as plt
from tqdm import tqdm

# Import delle nostre classi
from model import create_resnet50
from DataLoader import Create_DataLoader

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")


def transfer_weights_from_pretrained(custom_model, pretrained_model):
    “”"
    Transfers weights from pre-trained ResNet50 to our custom implementation.
    “”"
    print(" Weight transfer from ResNet50 pre-trained...")

    # Dictionaries for parameters
    custom_state = custom_model.state_dict()
    pretrained_state = pretrained_model.state_dict()

    # Mapping dei layer (escluso FC finale)
    weight_mapping = {}

    # Conv1 + BN1
    weight_mapping['conv1.weight'] = 'conv1.weight'
    weight_mapping['batchnorm1.weight'] = 'bn1.weight'
    weight_mapping['batchnorm1.bias'] = 'bn1.bias'
    weight_mapping['batchnorm1.running_mean'] = 'bn1.running_mean'
    weight_mapping['batchnorm1.running_var'] = 'bn1.running_var'

    #ResNet blocks (layer1, layer2, layer3, layer4)
    block_mapping = ['block1', 'block2', 'block3', 'block4']
    layer_mapping = ['layer1', 'layer2', 'layer3', 'layer4']

    for block_idx, (custom_block, pretrained_layer) in enumerate(zip(block_mapping, layer_mapping)):
        # Number of bottlenecks for each layer.
        num_bottlenecks = [3, 4, 6, 3][block_idx]

        for bottleneck_idx in range(num_bottlenecks):
            # Prefixes for layers.
            custom_prefix = f"{custom_block}.{bottleneck_idx}."
            pretrained_prefix = f"{pretrained_layer}.{bottleneck_idx}."

            # Conv layers in the bottleneck
            conv_mappings = [
                ('conv1_1x1', 'conv1'),
                ('conv2_3x3', 'conv2'),
                ('conv3_1x1', 'conv3')
            ]

            bn_mappings = [
                ('batchnorm1', 'bn1'),
                ('batchnorm2', 'bn2'),
                ('batchnorm3', 'bn3')
            ]

            # Transfer conv and batchnorm
            for custom_conv, pretrained_conv in conv_mappings:
                weight_mapping[f"{custom_prefix}{custom_conv}.weight"] = f"{pretrained_prefix}{pretrained_conv}.weight"

            for custom_bn, pretrained_bn in bn_mappings:
                weight_mapping[f"{custom_prefix}{custom_bn}.weight"] = f"{pretrained_prefix}{pretrained_bn}.weight"
                weight_mapping[f"{custom_prefix}{custom_bn}.bias"] = f"{pretrained_prefix}{pretrained_bn}.bias"
                weight_mapping[
                    f"{custom_prefix}{custom_bn}.running_mean"] = f"{pretrained_prefix}{pretrained_bn}.running_mean"
                weight_mapping[
                    f"{custom_prefix}{custom_bn}.running_var"] = f"{pretrained_prefix}{pretrained_bn}.running_var"

            # Downsample/projection if it exists
            if f"{pretrained_prefix}downsample.0.weight" in pretrained_state:
                weight_mapping[f"{custom_prefix}projection.0.weight"] = f"{pretrained_prefix}downsample.0.weight"
                weight_mapping[f"{custom_prefix}projection.1.weight"] = f"{pretrained_prefix}downsample.1.weight"
                weight_mapping[f"{custom_prefix}projection.1.bias"] = f"{pretrained_prefix}downsample.1.bias"
                weight_mapping[
                    f"{custom_prefix}projection.1.running_mean"] = f"{pretrained_prefix}downsample.1.running_mean"
                weight_mapping[
                    f"{custom_prefix}projection.1.running_var"] = f"{pretrained_prefix}downsample.1.running_var"

    # Transfer weights
    transferred = 0
    skipped = 0

    for custom_key, pretrained_key in weight_mapping.items():
        if custom_key in custom_state and pretrained_key in pretrained_state:
            if custom_state[custom_key].shape == pretrained_state[pretrained_key].shape:
                custom_state[custom_key] = pretrained_state[pretrained_key].clone()
                transferred += 1
            else:
                print(
                    f"  Shape mismatch: {custom_key} {custom_state[custom_key].shape} vs {pretrained_state[pretrained_key].shape}")
                skipped += 1
        else:
            skipped += 1

    # Load modified weights
    custom_model.load_state_dict(custom_state)

    print(f" Transferred: {transferred} parameters")
    print(f" Skipped: {skipped} parameters")
    print(f" FC layer initialized randomly for 101 classes")

    return custom_model


def sanity_check(model, train_loader, device):
    “”"
    Verify that the model is working properly.
    “”"
    print(" Sanity check of the model...")

    model.eval()
    with torch.no_grad():
        # Prendi un batch
        images, labels = next(iter(train_loader))
        images, labels = images.to(device), labels.to(device)

        print(f" Batch shape: {images.shape}")
        print(f" Labels shape: {labels.shape}")
        print(f" Labels range: {labels.min().item()} - {labels.max().item()}")

        # Forward pass
        outputs = model(images)
        print(f" Output shape: {outputs.shape}")
        print(f" Output range: {outputs.min().item():.3f} - {outputs.max().item():.3f}")

        # Softmax verification
        probabilities = torch.softmax(outputs, dim=1)
        print(f" Probabilities sum: {probabilities.sum(dim=1).mean().item():.6f}")

        # Loss calculation
        criterion = nn.CrossEntropyLoss()
        loss = criterion(outputs, labels)
        print(f" Initial loss: {loss.item():.4f}")

    print(" Sanity check completed!")
    return True


def train_epoch(model, train_loader, criterion, optimizer, device):
    “”"
    Training for a single era.
    “”"
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(train_loader, desc="Training")
    for batch_idx, (images, labels) in enumerate(pbar):
        images, labels = images.to(device), labels.to(device)

        # Zero gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass
        loss.backward()
        optimizer.step()

        # Statistics
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # Update progress bar
        if batch_idx % 50 == 0:
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100. * correct / total:.2f}%'
            })

    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total

    return epoch_loss, epoch_acc


def validate_epoch(model, val_loader, criterion, device):
    “”"
    Validation for a single era.
    “”"
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        pbar = tqdm(val_loader, desc="Validation")
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100. * correct / total:.2f}%'
            })

    epoch_loss = running_loss / len(val_loader)
    epoch_acc = 100. * correct / total

    return epoch_loss, epoch_acc


def plot_training_curves(train_losses, val_losses, train_accs, val_accs, save_path='training_curves.png'):
    “”"
    Plot the training and validation curves.
    “”"
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # Loss plot
    epochs = range(1, len(train_losses) + 1)
    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)
    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Accuracy plot
    ax2.plot(epochs, train_accs, 'b-', label='Training Accuracy', linewidth=2)
    ax2.plot(epochs, val_accs, 'r-', label='Validation Accuracy', linewidth=2)
    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

    print(f" Graphs saved in: {save_path}")


def main():
    # Configurazione
    FOOD101_ROOT = r"C:\Users\Mattia\Desktop\PROGETTO_COMPUTER_VISION\FOOD_101\food-101\food-101"  # Update this path
    SVHN_ROOT = r"C:\Users\Mattia\Desktop\PROGETTO_COMPUTER_VISION\SVHN"  # Update this path
    BATCH_SIZE = 64
    NUM_WORKERS = 0
    LEARNING_RATE = 0.001
    NUM_EPOCHS = 20
    SAVE_PATH = "resnet50_food101.pth"

    print(" Training ResNet50 on Food-101 Dataset")
    print("=" * 50)

    # Crea dataloader
    print(" Loading dataset...")
    train_loader, val_loader, test_loader, ood_loader = Create_DataLoader(
        food101_root=FOOD101_ROOT,
        svhn_root=SVHN_ROOT,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    print(f" Train batches: {len(train_loader)}")
    print(f" Val batches: {len(val_loader)}")
    print(f" Test batches: {len(test_loader)}")

    # Crea modello custom
    print("Custom model creation...")
    model = create_resnet50(num_classes=101).to(device)

    # Charge pre-trained model
    print(" Loading ResNet50 pre-trained...")
    pretrained_model = models.resnet50(pretrained=True)

    # Transfer weights# Load pre-trained model.
    model = transfer_weights_from_pretrained(model, pretrained_model)

    # Sanity check
    sanity_check(model, train_loader, device)

    '''
    # Setup training with adam
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    '''

    # Setup training with sgd
    criterion = nn.CrossEntropyLoss()

    # SGD Optimizer (instead of Adam)
    optimizer = optim.SGD(
        model.parameters(),
        lr=0.01,  # 10x higher than Adam
        momentum=0.9,  # Essential for SGD
        weight_decay=1e-4,
        nesterov=True  # Accelerated gradient
    )

    # CosineAnnealing (better for SGD than StepLR)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=NUM_EPOCHS,  # Total epochs
        eta_min=1e-6  # Minimum LR
    )
    # Lists for tracking
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    best_val_acc = 0.0

    print(f"\n Beginning training for {NUM_EPOCHS} epochs...")
    print("=" * 50)

    for epoch in range(NUM_EPOCHS):
        print(f"\n Epoch {epoch + 1}/{NUM_EPOCHS}")
        print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")

        # Training
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)

        # Validation
        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)

        # Scheduler step
        scheduler.step()

        # Save metrics
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        print(f" Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f" Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")

        # Save the best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_val_acc': best_val_acc,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'train_accs': train_accs,
                'val_accs': val_accs
            }, SAVE_PATH)
            print(f" New best model saved! Val Acc: {best_val_acc:.2f}%")

    print("\n Training completed!")
    print(f" Best Validation Accuracy: {best_val_acc:.2f}%")

    # Plot results
    print("\n Graph generation...")
    plot_training_curves(train_losses, val_losses, train_accs, val_accs)

    print(f"\n Model saved in: {SAVE_PATH}")
    print("Training successfully completed!")


if __name__ == "__main__":
    main()
