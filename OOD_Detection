import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, roc_curve
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from model import create_resnet50, create_feature_vae, ResNetFeatureExtractor
from DataLoader import Create_DataLoader

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#load the two models and extract the threshold found cpn the vae validation
def load_trained_models(resnet_path, vae_path):
    """Carica ResNet50 e VAE allenati + threshold"""
    print(" Loading trained models...")

    # Load ResNet50
    resnet_model = create_resnet50(num_classes=101).to(device)
    resnet_checkpoint = torch.load(resnet_path, map_location=device)
    resnet_model.load_state_dict(resnet_checkpoint['model_state_dict'])
    resnet_model.eval()

    # Load VAE
    vae_model = create_feature_vae(latent_dim=256).to(device)
    vae_checkpoint = torch.load(vae_path, map_location=device)
    vae_model.load_state_dict(vae_checkpoint['vae_state_dict'])
    vae_model.eval()

    # Extracting threshold
    threshold = vae_checkpoint['best_threshold']

    feature_extractor = ResNetFeatureExtractor(resnet_model)

    print(f" Models loaded! Threshold: {threshold:.4f}")
    return resnet_model, vae_model, feature_extractor, threshold

#I use resnet to extract features from 3 layer--> dim=1024
def extract_features_from_loader(model, feature_extractor, dataloader, max_samples=None):
    “”“Extracts features from a dataloader”“”"
    model.eval()
    features_list = []
    labels_list = []

    with torch.no_grad():
        for batch_idx, batch_data in enumerate(tqdm(dataloader, desc="Extracting features", leave=False)):
            if max_samples and batch_idx * dataloader.batch_size >= max_samples:
                break

            # Handle different batch formats (Food-101 vs SVHN)
            if len(batch_data) == 2:  # Normal case
                images, labels = batch_data
                ood_labels = labels  # Will be overridden later
            else:  # SVHN case with OOD marker
                images, ood_labels, _ = batch_data

            images = images.to(device)
            features = feature_extractor.get_features(images)

            features_list.append(features.cpu())
            labels_list.append(ood_labels)

    return torch.cat(features_list, dim=0), torch.cat(labels_list, dim=0)

#mix the datasets to create something more realistic ahahh
def create_mixed_ood_dataset(food_features, food_labels, svhn_features, svhn_labels):
    “”“Create mixed dataset for realistic OOD detection”“”
    # Food-101 = ID (label = 0), SVHN = OOD (label = 1)
    food_ood_labels = torch.zeros(len(food_features))  # ID = 0
    svhn_ood_labels = torch.ones(len(svhn_features))  # OOD = 1

    # Combine everything
    all_features = torch.cat([food_features, svhn_features], dim=0)
    all_ood_labels = torch.cat([food_ood_labels, svhn_ood_labels], dim=0)

    # Shuffle for realistic testing
    indices = torch.randperm(len(all_features))
    mixed_features = all_features[indices]
    mixed_labels = all_ood_labels[indices]

    print(f" Mixed dataset created:")
    print(f"   Food-101 (ID): {len(food_features)} samples")
    print(f"   SVHN (OOD): {len(svhn_features)} samples")
    print(f"   Total mixed: {len(mixed_features)} samples")

    return mixed_features, mixed_labels

#function that evaluates whether ood detection was good or not
def evaluate_ood_detection(vae, features, true_labels, threshold, batch_size=512):
    “”“Evaluate OOD detection and calculate all metrics”“”
    vae.eval()
    all_errors = []

    # Calculate reconstruction errors
    with torch.no_grad():
        num_batches = len(features) // batch_size + (1 if len(features) % batch_size else 0)

        for i in tqdm(range(num_batches), desc="Computing errors", leave=False):
            start_idx = i * batch_size
            end_idx = min(start_idx + batch_size, len(features))
            batch_features = features[start_idx:end_idx].to(device)

            errors = vae.reconstruction_error(batch_features)
            all_errors.append(errors.cpu())

    all_errors = torch.cat(all_errors, dim=0).numpy()

    # Threshold-based predictions.
    predictions = (all_errors > threshold).astype(int)  # 1 = OOD, 0 = ID
    true_labels = true_labels.numpy()

    return all_errors, predictions, true_labels

#calculating metrics
def calculate_comprehensive_metrics(errors, predictions, true_labels, threshold):
    “”“Calculate all OOD detection metrics”“”"
    # Confusion Matrix
    tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()

    # Basic metrics
    accuracy = accuracy_score(true_labels, predictions)
    precision = precision_score(true_labels, predictions)
    recall = recall_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions)

    # Metriche OOD specifiche
    auroc = roc_auc_score(true_labels, errors)

    # AUPR
    precision_curve, recall_curve, _ = precision_recall_curve(true_labels, errors)
    aupr = auc(recall_curve, precision_curve)

    # FPR@95TPR (standard in OOD detection)
    fpr, tpr, _ = roc_curve(true_labels, errors)
    fpr_at_95_tpr = fpr[np.argmax(tpr >= 0.95)] if np.max(tpr) >= 0.95 else 1.0

    # Detection Error
    detection_error = (fp + fn) / (tp + tn + fp + fn)

    return {
        # Confusion Matrix
        'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn,

        # Classification metrics
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1_Score': f1,

        # OOD specific metrics
        'AUROC': auroc,
        'AUPR': aupr,
        'FPR@95TPR': fpr_at_95_tpr,
        'Detection_Error': detection_error,

        # Additional info
        'Threshold': threshold,
        'Total_Samples': len(true_labels)
    }


def plot_ood_results(errors, true_labels, threshold, metrics):
    “”“Plot results OOD detection”“”"
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

    # 1. histogram of errors
    id_errors = errors[true_labels == 0]
    ood_errors = errors[true_labels == 1]

    ax1.hist(id_errors, bins=50, alpha=0.7, label=f'Food-101 (ID) - {len(id_errors)}', color='blue', density=True)
    ax1.hist(ood_errors, bins=50, alpha=0.7, label=f'SVHN (OOD) - {len(ood_errors)}', color='red', density=True)
    ax1.axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold: {threshold:.4f}')
    ax1.set_xlabel('Reconstruction Error')
    ax1.set_ylabel('Density')
    ax1.set_title(' Error Distribution: ID vs OOD')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. ROC Curve
    fpr, tpr, _ = roc_curve(true_labels, errors)
    ax2.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC: {metrics["AUROC"]:.4f})')
    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    ax2.set_xlabel('False Positive Rate')
    ax2.set_ylabel('True Positive Rate')
    ax2.set_title(' ROC Curve')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. Precision-Recall Curve
    precision_curve, recall_curve, _ = precision_recall_curve(true_labels, errors)
    ax3.plot(recall_curve, precision_curve, linewidth=2, label=f'PR (AUC: {metrics["AUPR"]:.4f})')
    ax3.set_xlabel('Recall')
    ax3.set_ylabel('Precision')
    ax3.set_title('Precision-Recall Curve')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. Confusion Matrix Visualization
    cm = np.array([[metrics['TN'], metrics['FP']],
                   [metrics['FN'], metrics['TP']]])
    im = ax4.imshow(cm, interpolation='nearest', cmap='Blues')
    ax4.set_title(' Confusion Matrix')

    # Add text annotations
    for i in range(2):
        for j in range(2):
            text = ax4.text(j, i, f'{cm[i, j]}', ha="center", va="center", fontsize=14, fontweight='bold')

    ax4.set_xticks([0, 1])
    ax4.set_yticks([0, 1])
    ax4.set_xticklabels(['Predicted ID', 'Predicted OOD'])
    ax4.set_yticklabels(['True ID', 'True OOD'])

    plt.tight_layout()
    plt.savefig('ood_detection_results.png', dpi=300, bbox_inches='tight')
    plt.show()

#----------------------------------- verification function --------------------------------------------
def verify_results(errors, true_labels, threshold):
    """Verification checks"""

    # 1. Basic stats
    food_errors = errors[true_labels == 0]  # ID
    svhn_errors = errors[true_labels == 1]  # OOD

    print(" VERIFICATION CHECKS:")
    print(
        f"Food-101 errors - Min: {food_errors.min():.6f}, Max: {food_errors.max():.6f}, Mean: {food_errors.mean():.6f}")
    print(
        f"SVHN errors    - Min: {svhn_errors.min():.6f}, Max: {svhn_errors.max():.6f}, Mean: {svhn_errors.mean():.6f}")
    print(f"Threshold: {threshold:.6f}")

    # 2. Check separation
    food_above_threshold = (food_errors > threshold).sum()
    svhn_below_threshold = (svhn_errors <= threshold).sum()

    print(
        f"Food samples above threshold: {food_above_threshold}/{len(food_errors)} ({100 * food_above_threshold / len(food_errors):.1f}%)")
    print(
        f"SVHN samples below threshold: {svhn_below_threshold}/{len(svhn_errors)} ({100 * svhn_below_threshold / len(svhn_errors):.1f}%)")

    # 3. Check for data leaks
    if len(food_errors) == 0 or len(svhn_errors) == 0:
        print(" ERROR: Missing data!")

    # 4. Sanity check - overlap
    overlap = len(set(food_errors.tolist()) & set(svhn_errors.tolist()))
    print(f"Overlapping error values: {overlap} (should be 0 ideally)")

#---------------------------------------------------------------------------------------------------------

def print_detailed_results(metrics):
    """Stampa risultati dettagliati"""
    print("\n" + "=" * 60)
    print(" OOD DETECTION RESULTS")
    print("=" * 60)

    print(" CONFUSION MATRIX:")
    print(f"   True Negatives (ID→ID):   {metrics['TN']:>6}")
    print(f"   False Positives (ID→OOD): {metrics['FP']:>6}")
    print(f"   False Negatives (OOD→ID): {metrics['FN']:>6}")
    print(f"   True Positives (OOD→OOD): {metrics['TP']:>6}")

    print(f"\n CLASSIFICATION METRICS:")
    print(f"   Accuracy:      {metrics['Accuracy']:.4f} ({metrics['Accuracy'] * 100:.2f}%)")
    print(f"   Precision:     {metrics['Precision']:.4f}")
    print(f"   Recall:        {metrics['Recall']:.4f}")
    print(f"   F1-Score:      {metrics['F1_Score']:.4f}")

    print(f"\n OOD DETECTION METRICS:")
    print(f"   AUROC:         {metrics['AUROC']:.4f}")
    print(f"   AUPR:          {metrics['AUPR']:.4f}")
    print(f"   FPR@95TPR:     {metrics['FPR@95TPR']:.4f}")
    print(f"   Detection Err: {metrics['Detection_Error']:.4f}")

    print(f"\n⚙  CONFIGURATION:")
    print(f"   Threshold:     {metrics['Threshold']:.4f}")
    print(f"   Total Samples: {metrics['Total_Samples']:,}")


def main():
    # Configurazione
    FOOD101_ROOT = r"C:\Users\Mattia\Desktop\PROGETTO_COMPUTER_VISION\FOOD_101\food-101\food-101"
    SVHN_ROOT = r"C:\Users\Mattia\Desktop\PROGETTO_COMPUTER_VISION\SVHN"
    RESNET_PATH = r"C:\Users\Mattia\Desktop\PROGETTO_COMPUTER_VISION\SGD\resnet50_food101.pth"
    VAE_PATH = r"C:\Users\Mattia\Desktop\PROGETTO_COMPUTER_VISION\SGD\vae\feature_vae_military.pth"

    print(" OOD DETECTION EVALUATION")
    print("=" * 50)

    # Load modells
    resnet_model, vae_model, feature_extractor, threshold = load_trained_models(RESNET_PATH, VAE_PATH)

    # Crea dataloaders
    _, _, food_test_loader, svhn_test_loader = Create_DataLoader(
        food101_root=FOOD101_ROOT,
        svhn_root=SVHN_ROOT,
        batch_size=64,
        num_workers=0
    )

    # Extract feature
    print(" Extracting features from test sets...")
    food_features, food_labels = extract_features_from_loader(resnet_model, feature_extractor, food_test_loader)
    svhn_features, svhn_labels = extract_features_from_loader(resnet_model, feature_extractor, svhn_test_loader)

    # Create mixed dataset
    mixed_features, mixed_ood_labels = create_mixed_ood_dataset(
        food_features, food_labels, svhn_features, svhn_labels
    )

    # Currency OOD detection
    print(" Evaluating OOD detection...")
    errors, predictions, true_labels = evaluate_ood_detection(
        vae_model, mixed_features, mixed_ood_labels, threshold
    )

    # Calculate metrics
    metrics = calculate_comprehensive_metrics(errors, predictions, true_labels, threshold)

    #verify the results
    verify_results(errors, true_labels, threshold)

    # Show results
    print_detailed_results(metrics)

    # Plot results
    print("\n Generating plots...")
    plot_ood_results(errors, true_labels, threshold, metrics)

    print("\n OOD DETECTION EVALUATION COMPLETE!")


if __name__ == "__main__":
    main()
